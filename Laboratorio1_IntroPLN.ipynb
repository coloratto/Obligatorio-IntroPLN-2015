{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se importan las librerias correspondientes\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas\n",
    "import numpy as np\n",
    "import utils_pln as utl # Archivo con utilitarios, los mismos se separaron del notebook para no sobrecargarlo\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import cross_validation\n",
    "import datetime\n",
    "\n",
    "print(\"START: \" + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importamos el corpus de comentarios\n",
    "\n",
    "Pandas es una biblioteca de código abierto implementada en Python la cual permite realizar una fácil manipulación y análisis de los datos.\n",
    "Ésta se utilizó para cargar los datos en memoria y realizar un breve análisis de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se carga en 'datos' el archivo csv en memoria\n",
    "datos = pandas.read_csv(\"comentarios_peliculas.csv\", skiprows=1, delimiter=';', skip_blank_lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Características del corpus\n",
    "Para conocer acerca del corpus sobre el cual se trabajará se realiza un breve análisis de los datos. Para ello se obtiene la cantidad de filas y columnas que el dataset posee; donde las filas corresponden a los comentarios y las columnas a los atributos asociados a los mismos. Por otro lado se obtiene la cantidad de peliculas del corpus, la cantidad de comentarios para cada película, en el que se realiza un ploteo, entre otros. A su vez se listan esos atributos junto a su tipo asociado. Una vez hecho esto se procede a chequear de que los atributos de los comentarios tengan o no valores faltantes, ya que en caso de tenerlos hay que tomar una decisión en base a que hacer con los mismos (existen varias técnicas para este tratamiento). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se obtiene cantidad peliculas, de comentarios y cantidad de atributos\n",
    "cantPeliculas = len(datos.ix[:,0].unique())\n",
    "cantComentarios = len(datos.index)\n",
    "cantAtributos = len(datos.columns)\n",
    "\n",
    "# Se imprimen los datos obtenidos anteriormente junto a sus tipos de datos\n",
    "print (\"Hay una cantidad de \" + str(cantPeliculas) + \" películas en el corpus.\")\n",
    "print (\"El corpus posee \" + str(cantComentarios) + \" comentarios con \" + str(cantAtributos) + \" atributos por cada uno de ellos.\")\n",
    "\n",
    "# Se imprime la cantidad de comentarios para cada pelicula\n",
    "print (\"\\nCantidad de comentarios asociados a cada película:\\n\")\n",
    "print (datos.ix[:,0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se importa la libreria para utilizar plot\n",
    "# Además se plotea en el própio notebook y no como una ventana nueva\n",
    "%matplotlib inline\n",
    "from pylab import *\n",
    "\n",
    "# Se realiza una gráfica de cantidad de comentarios en función de su clasificación\n",
    "gruposCalificacion = datos.groupby([u'Calificación']).groups\n",
    "keyClasificacion = gruposCalificacion.keys()\n",
    "cantClasificacion = []\n",
    "for k in keyClasificacion:\n",
    "    cantClasificacion.append(len(gruposCalificacion.get(k)))    \n",
    "pos = arange(len(keyClasificacion)) + 0.5 \n",
    "\n",
    "figure(1)\n",
    "barh(pos,cantClasificacion, align='center')\n",
    "yticks(pos, list(keyClasificacion))\n",
    "xlabel('Cantidad de comentarios')\n",
    "ylabel(u'Clasificación')\n",
    "title(u'Grafica cantidad de comentarios por clasificación')\n",
    "grid(True)\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Se imprime la cantidad de comentarios asociados a cada clase (es decir cantidad de comentarios por calificación)\n",
    "print (\"\\nCantidad de comentarios asociados a cada calificación:\\n\")\n",
    "print (datos.ix[:,7].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"A continuación se listan los atributos con su tipo asociado:\\n\")\n",
    "print (datos.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se chequea si los atributos poseen o no valores faltantes\n",
    "# Para ello se recorren todos los atributos (columnas de 'datos')\n",
    "for atributo in datos.columns:\n",
    "    \n",
    "    # Se obtiene la cantidad de valores distinto de vacio del atributo \n",
    "    cantValoresAtributo = datos[atributo].describe()['count']\n",
    "    \n",
    "    # Si hay menos que la cantidad de comentarios de 'datos' -> hay valores faltantes\n",
    "    if(cantComentarios > cantValoresAtributo):\n",
    "        print (\"Hay valores faltantes para el atributo \" + atributo)\n",
    "    else:\n",
    "        print (\"No hay valores faltantes para el tributo \" + atributo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Descartamos columnas innecesarias\n",
    "En esta sección se descartan las columnas (atributos) que se consideran innecesarios. En particular las columnas que creemos se consideran necesarias son solamente las columnas de los comentarios y los puntajes asociados a dichos comentarios. Por lo que se procede a eliminar el resto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se descartan de 'datos' las columnas innecesarias, dejando solamente los comentarios y sus calificaciones\n",
    "datos.drop(datos.columns[[0,1,2,3,4,5,8]],inplace=True,axis=1)\n",
    "comentarios_peliculas = datos\n",
    "# Se imprime un resumen de los datos\n",
    "print (comentarios_peliculas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocesamiento de los comentarios\n",
    "El corpus sobre el cual se trabaja puede contener datos con ruido en su contenido. Es decir, diversos elementos que están mezclados en el contenido pero que además de no aportan nada a la tarea de clasificación, generan ruido dificultando las tareas posteriores. Para ello entonces se procede a realizar el preprocesamiento de los comentarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se limpian los datos y se los convierte a una lista de tuplas para su facil manipualacion.\n",
    "comentarios_peliculas = utl.convert_to_list(utl.depurar_comentarios(comentarios_peliculas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Separamos entrenamiento y testeo \n",
    "Una vez de haber llegado a este punto se divide dicha lista en conjuntos de entrenamiento y testeo, para primero entrenar el algoritmo y luego testearlo.\n",
    "Para poder entrenar y testear un algoritmo de aprendizaje, como se mencionó, es necesario primero particionar los datos en dos conjuntos disjuntos de entrenamiento y testeo. Se separarán aleatoriamente un 20% de los datos para testeo y el 80% restante se utilizará para el entrenamiento. Para esto se utilizó la función train_test_split del paquete cross_validation de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Se divide el conjunto de datos en train (80%) y test (20%)\n",
    "datos_train, datos_test = train_test_split(comentarios_peliculas, test_size=0.2)\n",
    "\n",
    "# Se chequean las cantidades de la división\n",
    "print (len(datos_train))\n",
    "print (len(datos_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenización\n",
    "En esta sección se realiza el proceso de tokenización de los textos utilizando la biblioteca NLTK. Además se convierte a un formato suceptible para la clasificación. Es decir, se toquenizan los comentarios y se calcula el diccionario de palabras frecuentes del mismo. En base a ello y al valor de clasificación asociado se crea una tupla la cual corresponde al comentario analizado. Luego estas tuplas (una por cada comentario) son la entrada para el entrenamiento del algoritmo Entropía Máxima provisto por NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datos_train_tokenizados_nltk = utl.tokenizar_nltk(datos_train)\n",
    "\n",
    "#Se imprime el primer comentario tokenizado, cada palabra aparece junto a la cantidad de veces\n",
    "# que ocurre en el comentario y ademas se imprime la clasificacion del comentario.\n",
    "print (datos_train_tokenizados[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Se entrena el algoritmo de máxima entropia y se lo evalua\n",
    "Una vez de tener los datos en un formato aceptable para los algoritmos de entrenamiento y particionado los mismos en test y train, se procede a realizar el entrenamiento del algoritmo de Máxima Entropía y se utiliza cross validation. Con cada clasificador de la cross validation se calcula el promedio de la acuracy. En este caso, es decir utilizando como features todas las palabras se obtiene un 84%.\n",
    "\n",
    "Obs: Este proceso es muy lento (aproximadamente 25 minutos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluacion del clasificador con los datos de test\n",
    "Luego del entrenamiento se procede a evaluar el clasificador con los datos de test, y se obtiene la matriz de confusion para poder analizar claramente los errores (Esto se realiza con la funcion getTasa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# En la siguiente linea se entrena el algoritmo con 8 iteraciones\n",
    "# Obs. El entrenamiento es bastante lento.\n",
    "clf_max_ent = nltk.classify.MaxentClassifier.train(datos_train_tokenizados_nltk,max_iter=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Se imprime la tasa de acierto y la matriz de confucion\n",
    "datos_test_tokenizado_nltk = utl.tokenizar_nltk(datos_test)\n",
    "utl.getTasa(clf_max_ent,datos_test_tokenizado_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede ver en la matriz de confucion los resultados aun son poco fiables, por ejemplo en el caso de los comentarios negativos la mayoria de los comentarios fueron mal clasificados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Palabras mas frecuentes\n",
    "En esta seccion se busca mejorar el clasificaodr utilizando como features las palabras mas frecuentes, para esto se cuentan las palabras del corpus y se calcula su frecuencia. Se probara el clasificador para distintas cantidadades de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cross valodation para el clasificador\n",
    "# Frecuencia -1 significa todas las palabras\n",
    "fecuencias = [-1, 5000, 3000, 2000, 1000, 500]\n",
    "\n",
    "accuracy_frec_cv = []\n",
    "\n",
    "for frec in fecuencias:\n",
    "    print('******************************************************************************')\n",
    "    print(\"FRECUENCIA: \" + str(frec))\n",
    "    \n",
    "    datos_train_frec = utl.filtrar(datos_train, utl.palabras_mas_frecuentes(frec,datos_train), False)\n",
    "    datos_train_tokenizados = utl.tokenizar_nltk(datos_train_frec)\n",
    "    \n",
    "    cv = cross_validation.KFold(len(datos_train_tokenizados), n_folds=10)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for traincv, evalcv in cv:\n",
    "        classifier = nltk.classify.MaxentClassifier.train(datos_train_tokenizados[traincv[0]:traincv[len(traincv)-1]],max_iter=8,trace=1)\n",
    "        accuracy += nltk.classify.util.accuracy(classifier, datos_train_tokenizados[evalcv[0]:evalcv[len(evalcv)-1]])\n",
    "        \n",
    "    accuracy_avg = (accuracy / 10)\n",
    "    accuracy_frec_cv += [(frec,accuracy_avg)]\n",
    "    print('ACCURACY: ' + str(accuracy_avg))\n",
    "print (accuracy_frec_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_frec = utl.getBestFrec(accuracy_frec_cv)\n",
    "palabras_mas_frecuentes = utl.palabras_mas_frecuentes(best_frec,datos_train)\n",
    "datos_train_frec = utl.filtrar(datos_train, palabras_mas_frecuentes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos_train_tokenizados_nltk = utl.tokenizar_nltk(datos_train_frec)\n",
    "    \n",
    "classifier = nltk.classify.MaxentClassifier.train(datos_train_tokenizados_nltk,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizado_nltk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementos subjetivos\n",
    "Se agregan los elementos subjetivos...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elementos_subjetivos = utl.codificarClasificacionesSubjetivos(utl.diccionarioElementosSubjetivos(\"elementos_subjetivos.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_nltk = datos_train_tokenizados_nltk + elementos_subjetivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classifier = nltk.classify.MaxentClassifier.train(features_nltk,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizado_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizacion con Freeling\n",
    "Se tokeniza con freeling y se repite el proceso..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos_train_tokenizados_freeling = utl.tokenizar_freeling(datos_train_frec)\n",
    "datos_test_tokenizados_freeling = utl.tokenizar_freeling(datos_test)\n",
    "\n",
    "features_freeling = datos_train_tokenizados_freeling + elementos_subjetivos\n",
    "\n",
    "classifier = nltk.classify.MaxentClassifier.train(features_freeling,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizados_freeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datos_train_tokenizados_pos_tagging = utl.POS_taggig(datos_train_frec)\n",
    "datos_test_tokenizados_pos_tagging = utl.POS_tagging(datos_test)\n",
    "\n",
    "features_POS_tagging = datos_train_tokenizados_POS_tagging + elementos_subjetivos\n",
    "\n",
    "classifier = nltk.classify.MaxentClassifier.train(features_POS_tagging,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizados_POS_tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opcionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dominio_cine_peliculas = open('terminosNoValorativosAmbitoCine.txt').read()\n",
    "caracteres_especiales = open('caracteres_especiales.txt').read()\n",
    "nltk_stopwords = stopwords.words('spanish')\n",
    "\n",
    "datos_train_frec = utl.filtrar(datos_train_frec, nltk_stopwords, True)\n",
    "datos_train_frec = utl.filtrar(datos_train_frec, caracteres_especiales, True)\n",
    "datos_train_frec = utl.filtrar(datos_train_frec, dominio_cine_peliculas, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NLTK\n",
    "datos_train_tokenizados_nltk = utl.tokenizar_nltk(datos_train_frec)\n",
    "datos_test_tokenizado_nltk = utl.tokenizar_nltk(datos_test)\n",
    "\n",
    "features_nltk = datos_train_tokenizados_nltk + elementos_subjetivos\n",
    "\n",
    "classifier = nltk.classify.MaxentClassifier.train(datos_train_tokenizados_nltk,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizado_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#FREELING\n",
    "datos_train_tokenizados_freeling = utl.tokenizar_freeling(datos_train_frec)\n",
    "datos_test_tokenizados_freeling = utl.tokenizar_freeling(datos_test)\n",
    "\n",
    "features_freeling = datos_train_tokenizados_freeling + elementos_subjetivos\n",
    "\n",
    "classifier = nltk.classify.MaxentClassifier.train(features_freeling,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizados_freeling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#POS-tagging\n",
    "datos_train_tokenizados_pos_tagging = utl.POS_taggig(datos_train_frec)\n",
    "datos_test_tokenizados_pos_tagging = utl.POS_tagging(datos_test)\n",
    "\n",
    "features_POS_tagging = datos_train_tokenizados_POS_tagging + elementos_subjetivos\n",
    "\n",
    "classifier = nltk.classify.MaxentClassifier.train(features_POS_tagging,max_iter=8,trace=1)\n",
    "utl.getTasa(classifier,datos_test_tokenizados_POS_tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"END: \" + str(datetime.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
